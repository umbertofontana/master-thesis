{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installazione delle librerie nell'environment di Google Colab\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
    "!pip install optimum\n",
    "!pip install langchain\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install unstructured[pdf]\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb\n",
    "\n",
    "#Importazione delle librerie di interesse.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from langchain.vectorstores import Chroma #ChromaDB integrato con LangChain\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import re\n",
    "import os\n",
    "\n",
    "#Sezione di scraping del sito web delle Nazioni Unite per recuperare i documenti di interesse. In questo caso, si tratta di 50 documenti del Consiglio di Sicurezza dell'ONU in lingua inglese, aventi come argomento il conflitto Israelo-Palestinese\n",
    "url = \"https://digitallibrary.un.org/search?ln=en&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=0&fct__1=Meeting%20Records&fct__2=Security%20Council&fct__3=2023&fti=0&p=\"\n",
    "data = requests.get(url)\n",
    "soup = BeautifulSoup(data.content)\n",
    "articles = soup.select('a.moreinfo')\n",
    "cleanlinks = []\n",
    "#Creazione di una lista composta da tutti gli URL presenti nella pagina iniziale\n",
    "links = []\n",
    "for a_tag in soup.find_all('a', class_=\"moreinfo\", href=True):\n",
    "    links.append(a_tag['href'])\n",
    "#Creazione di una lista composta da tutti gli URL che puntano ad un PDF\n",
    "for i in links:\n",
    "    if \"record\" in i:\n",
    "        cleanlinks.append(i)      \n",
    "#Apertura di ogni singolo URL ottenuto e recupero dei file PDF di interesse, salvandoli in una cartella locale.\n",
    "documentspage = []\n",
    "base_url = \"https://digitallibrary.un.org\"\n",
    "for i in cleanlinks:\n",
    "    request_href = base_url + str(i)\n",
    "    minidata = requests.get(request_href)\n",
    "    minisoup = BeautifulSoup(minidata.content)\n",
    "    for j in minisoup.find_all('meta', content=True):\n",
    "        if \"EN\" in str(j):\n",
    "            substring = str(j).split(\"content=\\\"\")[1]\n",
    "            substring = substring.split(\"\\\" name\")[0]\n",
    "            os.system('wget %s' % substring)\n",
    "            print(\"Done\")\n",
    "#Percorso in cui sono contenuti i file PDF scaricati.\n",
    "directory = \"/content/PDF\"\n",
    "#Creazione di una funzione che carichi i documenti dalla cartella locale.\n",
    "def docloader(directory):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "#Creazione di una funzione che splitti i documenti per non superare i limiti di embedding.\n",
    "def docsplitter(documents, chunk_size=1000, chunk_overlap=20):\n",
    "    textsplitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    splitteddocs = textsplitter.split_documents(documents)\n",
    "    return splitteddocs\n",
    "#Caricamento della cartella precedentemente specificata e vettorizzazione dei documenti utilizzando SentenceTransformer\n",
    "documents = docloader(directory)\n",
    "splitteddocs = docsplitter(documents)\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\") #V. https://docs.trychroma.com/embeddings\n",
    "\n",
    "#Creazione del database vettoriale\n",
    "db = Chroma.from_documents(splitteddocs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inizializzazione del modello StableBeluga a 7B ottimizzato tramite GPTQ\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/StableBeluga-7B-GPTQ\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "#Inizializzazione del tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/StableBeluga-7B-GPTQ\")\n",
    "#Creazione di una funzione che formatti l'input in modo da inserire la domanda posta e il relativo contesto da fornire al modello\n",
    "def ask(syst, context, question):\n",
    "    return f\"\"\"### System:\\n{syst}\\n\\n### User:\\n\\n## Context\\n\\n{context}\\n\\n## Question\\n\\n {question}\\n\\n### Assistant:\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione di una funzione che interroghi il modello e restituisca la risposta\n",
    "def invoke(p, maxlen, sample=True):\n",
    "  tokens = tokenizer(p, return_tensors=\"pt\")\n",
    "  answer = model.generate(**tokens.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample).to(\"cpu\")\n",
    "  answer = str(tokenizer.batch_decode(answer))\n",
    "  return answer.split(\"### Assistant:\\\\n \",1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definizione della funzione di ricerca semantica\n",
    "def search(question, k):\n",
    "    simsearch = db.similarity_search_with_relevance_scores(question, k)\n",
    "    return simsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formato di interrogazione del modello.\n",
    "%%time\n",
    "#Definizione delle custom instructions. In questo caso, viene imposto al modello di rispondere alla domanda posta solamente utilizzo le informazioni fornite come contesto; nel caso in cui trovi nulla di attinente, non deve cercare di rispondere comunque, ma dire chiaramente di non essere in grado di rispondere. In questo modo, si cerca di evitare il problema delle allucinazioni.\n",
    "syst = \"You are Stable Beluga, an AI that follows instructions and helps the user as much as it can. Please answer the question with information from the context given. If there isn't enough information in the context, don't try to answer and just say it.\"\n",
    "question = \"How is the European Unione contributing the resolution of the Palestinian conflict?\"\n",
    "#Contesto fornito al modello per la generazione della risposta, ottenuto tramite la funzione di ricerca semantica\n",
    "context = search(question, 15)\n",
    "invoke(ask(syst, context, question), 1000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
